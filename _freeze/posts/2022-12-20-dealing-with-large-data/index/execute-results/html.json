{
  "hash": "043ea6ea47a8d62da23b9e97b1c81078",
  "result": {
    "markdown": "---\ntitle: \"Strategies for dealing with large data\"\nauthor: \n  - name: Stephanie Hicks\n    url: https://stephaniehicks.com\n    affiliation: Department of Biostatistics, Johns Hopkins\n    affiliation_url: https://publichealth.jhu.edu\ndescription: \"Introduction to basic strategies for dealing with large data in R\"\ndate: 2022-12-20\ncategories: [module 5, week 9, large data, programming, R]\n---\n\n\n# Pre-lecture materials\n\n### Acknowledgements\n\nMaterial for this lecture was borrowed and adopted from\n\n- [A great blog post from 2019 by Alex Gold from RStudio](https://rviews.rstudio.com/2019/07/17/3-big-data-strategies-for-r/).\n- <https://www.stephaniehicks.com/jhustatcomputing2021/posts/2021-10-12-dealing-with-large-data>\n\n# Learning objectives\n\n::: callout-note\n# Learning objectives\n\n**At the end of this lesson you will:**\n\n- Recognize different file formats to work with large data not locally\n- Implement three ways to work with large data: \n  1. \"sample and model\" \n  2. \"chunk and pull\"\n  3. \"push compute to data\"\n\n:::\n\n\n# Introduction \n\nFirst, we load a few R packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(here)\nlibrary(dbplyr)\nlibrary(rsample)\n```\n:::\n\n\nFor most data analyses in R, data you encounter can **easily be read into memory** in R (either locally or on a cluster of sorts) and analyzed in a standard way. \n\nHowever, if you do encounter data that is too big to be read into memory, you might start to **search for strategies** on how to deal with this data. \n\nFor most of people, it **might be obvious why** you would want to use R with big data, but it\n**not obvious how**. \n\nNow, you might say advances in hardware make this less and less of a problem as most laptops come with >8-32Gb of memory and it is easy to get instances on cloud providers with terabytes of RAM. \n\nThat's definitely true. But there might be some problems that you will run into. \n\n## Loading data into memory\n\nLet's say **you are able load part of the data into the RAM** on your machine (in-memory).\n\nIf you had something like a zipped `.csv` file, you could always \ntry loading just the first few lines into memory (see `n_max = 8` below) \nto see what is inside the files, but **eventually you will likely need a different strategy**. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_csv(readr_example(\"mtcars.csv.bz2\"), \n         skip = 0, n_max = 8, progress = show_progress())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n```\n:::\n:::\n\n\n\n## Memory for calculations \n\nYou have to keep in mind that you will need to do something with the data too (typically need 2-3\ntimes the RAM of the size of your data). \n\nThis may or may not be a problem for your hardware that you are working with. \n\n## Transfer speeds can be slow\n\nIf you are working with data on a server that needs to be transferred somewhere to do the processing or computation once the data has been transferred.\n\nFor example, the time it takes to make a call over the internet \nfrom San Francisco to New York City takes over 4 times longer \nthan reading from a standard hard drive and over \n[200 times longer than reading from a solid state hard drive](https://blog.codinghorror.com/the-infinite-space-between-words/). \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://blog.codinghorror.com/content/images/2014/May/internet-latency-usa.png){width=90%}\n:::\n:::\n\n\n[[image source](https://blog.codinghorror.com/content/images/2014/May/internet-latency-usa.png)]\n\nThis is an especially **big problem early in developing a model** or performing \na data analysis, when data might have to be pulled repeatedly.\n\nToday we are going to discuss some strategies (and R packages) for \nworking with big data in R. We will also go through some examples of \nhow to execute these strategies in R. \n\n# Data \n\nWe will use the [`nycflights13`](https://github.com/hadley/nycflights13) \ndata that we learned about previously.\n\nWhat's in the data package?\n\n> \"This package contains information about all flights that departed from NYC (e.g. EWR, JFK and LGA) to destinations in the United States, Puerto Rico, and the American Virgin Islands) in 2013: 336,776 flights in total. To help understand what causes delays, it also includes a number of other useful datasets.\"\n\nThis package provides the following data tables.\n\n- `flights`: all flights that departed from NYC in 2013\n- `weather`: hourly meterological data for each airport\n- `planes`: construction information about each plane\n- `airports`: airport names and locations\n- `airlines`: translation between two letter carrier codes and names\n\nHowever, this time we will cache the data from the `nycflights13` \npackage in a form we are already familiar with (SQLite databases). \nBut there are many other data formats that you might encounter including: \n\n- `.sqlite` (SQL database). Talk more about this in a bit. \n- `.csv` (comma separated values). Good for storing rectangular data. However, can really slow to read and write, making them (often) unusable for large datasets. \n- `.json` (JavaScript object notation). Key-value pairs in a partially structured format\n- `.parquet` (Apache Parquet). Developed by Cloudera and Twitter to serve as a column-based storage format, optimized for work with multi-column datasets. Can be used for [Spark data](http://spark.apache.org) or other tools in the Hadoop ecosystem. When you store data in parquet format, you actually get a whole directory worth of files. The data is split across multiple `.parquet` files, allowing it to be easily stored on multiple machines, and there are some metadata files too, describing the contents of each column. Can use [`sparklyr`](https://spark.rstudio.com) to import `.parquet` files\n- `.avro` (Apache Avro). Released by the Hadoop working group in 2009. It is a row-based format that is highly splittable. It is also described as a data serialization system similar to Java Serialization. The schema is stored in JSON format, while the data is stored in binary format, minimizing file size and maximizing efficiency. Can use [`sparkavro`](https://cran.r-project.org/web/packages/sparkavro/index.html) to import `.avro` files. \n- `.zarr` (Zarr). [Zarr files](https://zarr.readthedocs.io/en/stable/) are a modern library and data format for storing chunked, compressed N-dimensional data in Python, but can work with these files using reticulate. Still very much in development though. \n- `.h5` (Hierarchical Data Format or HDF5). Mature (20 years old) library and data format which is also designed to handle chunked compressed N-dimensional data. Can use [`rhdf5`](https://www.bioconductor.org/packages/rhdf5) and [`HDF5Array`](https://www.bioconductor.org/packages/HDF5Array) to read and write `.h5` files.\n\n## SQLite databases\n\nOK so as mentioned above, let's use the SQLite format to demonstrate the \nstrategies for dealing with large data. However, they can easily transfer \nother data formats. \n\n**Reminder**: There are several ways to \n[query](https://db.rstudio.com/getting-started/database-queries/)\n`SQL` or `SQLite` databases in R. \n\nOk, we will set up the SQLite database using the \n`nycflights13_sqlite()` function in the `dbplyr` package. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nycflights13)\nif(!file.exists(here(\"data\", \"nycflights13\", \"nycflights13.sqlite\"))){\n  dir.create(here(\"data\", \"nycflights13\"))\n  dbplyr::nycflights13_sqlite(path=here(\"data\", \"nycflights13\"))\n}\n```\n:::\n\n\nWe can check to see what file has been created\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist.files(here(\"data\", \"nycflights13\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"nycflights13.sqlite\"\n```\n:::\n:::\n\n\n:::{.callout-note}\n\n### Question\n\nHow can we use the `DBI::dbConnect()` function with \n`RSQLite::SQLite()` backend to connect to the `SQLite` database?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DBI)\n# try it yourself \n```\n:::\n\n\n<details> <summary> Click here for the answer.</summary>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DBI)\nconn <- dbConnect(RSQLite::SQLite(), \n                  here(\"data\", \"nycflights13\", \"nycflights13.sqlite\"))\nconn\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQLiteConnection>\n  Path: /Users/stephaniehicks/Documents/github/teaching/jhustatprogramming2022/data/nycflights13/nycflights13.sqlite\n  Extensions: TRUE\n```\n:::\n:::\n\n\n</details> \n\n:::\n\n\n:::{.callout-note}\n\n### Question\n\nNext, let's use the `dplyr::tbl()` function returns \nsomething that feels like a data frame with the `flights` dataset. \nFinally, show the first 10 rows of the data frame. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# try it yourself \n```\n:::\n\n\n<details> <summary> Click here for the answer.</summary>\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl(conn, \"flights\") %>%\n  head(n=10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:   SQL [10 x 19]\n# Database: sqlite 3.39.4 [/Users/stephaniehicks/Documents/github/teaching/jhustatprogramming2022/data/nycflights13/nycflights13.sqlite]\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      517        515       2     830     819      11 UA     \n 2  2013     1     1      533        529       4     850     830      20 UA     \n 3  2013     1     1      542        540       2     923     850      33 AA     \n 4  2013     1     1      544        545      -1    1004    1022     -18 B6     \n 5  2013     1     1      554        600      -6     812     837     -25 DL     \n 6  2013     1     1      554        558      -4     740     728      12 UA     \n 7  2013     1     1      555        600      -5     913     854      19 B6     \n 8  2013     1     1      557        600      -3     709     723     -14 EV     \n 9  2013     1     1      557        600      -3     838     846      -8 B6     \n10  2013     1     1      558        600      -2     753     745       8 AA     \n# … with 9 more variables: flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dbl>, and abbreviated variable names ¹​sched_dep_time,\n#   ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n```\n:::\n:::\n\n\n</details> \n\n:::\n\nBefore we jump into the next section, let's save this data frame as\n`flights_df` and count the number of rows using `dplyr::tally()`: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_df <- dplyr::tbl(conn, \"flights\")\nflights_df %>% \n  tally()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:   SQL [1 x 1]\n# Database: sqlite 3.39.4 [/Users/stephaniehicks/Documents/github/teaching/jhustatprogramming2022/data/nycflights13/nycflights13.sqlite]\n       n\n   <int>\n1 336776\n```\n:::\n:::\n\n\nEven though it only has a few hundred thousand rows, it is still \nuseful to demonstrate some strategies for dealing with big data \nin R. \n\n# Sample and Model \n\nThe first strategy is to downsample your data to a size that can \nbe downloaded (or if already downloaded, just loaded into memory) \nand perform your analysis on the downsampled data. This also allows \nmodels and methods to be run in a reasonable amount of time. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://rviews.rstudio.com/post/2019-07-01-3-big-data-paradigms-for-r_files/sample_model.png)\n:::\n:::\n\n\n[[image source](https://rviews.rstudio.com/post/2019-07-01-3-big-data-paradigms-for-r_files/sample_model.png)]\n\n\n:::{.callout-tip}\n\n### Note\n\nIf maintaining class balance is necessary (or one class\nneeds to be over/under-sampled), it is reasonably simple to \nstratify the data set during sampling.\n\n:::\n\n## Advantages \n\n- **Speed**. Relative to working on your entire data set, working on just a sample can drastically decrease run times and increase iteration speed.\n- **Prototyping**. Even if you will eventually have to run your model on the entire data set, this can be a good way to refine hyperparameters and do feature engineering for your model.\n- **Packages**. Since you are working on a regular, in-memory data set, you can use all your favorite R packages.\n\n\n## Disadvantages\n\n- **Sampling**. Downsampling is not terribly difficult, but does need to be done with care to ensure that the sample is valid and that you have pulled enough points from the original data set.\n- **Scaling**. If you are using sample and model to prototype something that will later be run on the full data set, you will need to have a strategy (such as pushing compute to the data) for scaling your prototype version back to the full data set.\n- **Totals**. [Business Intelligence](https://en.wikipedia.org/wiki/Business_intelligence) (BI) -- or  _strategies and technologies used by enterprises for the data analysis of business information_ (e.g. data mining, reporting, predictive analytics, etc) -- tasks frequently answer questions about totals, like the count of all sales in a month. One of the other strategies is usually a better fit in this case.\n\n\n## Example \n\nLet's say we want to model whether flights will be delayed or not.\nWe will start with some minor cleaning of the data. \n\nFirst, we will create a `is_delayed` column in the database: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_df <- \n  flights_df %>%\n    dplyr::mutate(is_delayed = arr_delay > 0,\n                  hour = sched_dep_time / 100) %>% # Get just hour (currently formatted so 6 pm = 1800)\n    # Remove small carriers that make modeling difficult\n    dplyr::filter(!is.na(is_delayed) & !carrier %in% c(\"OO\", \"HA\"))\n```\n:::\n\n\nHere are the total number of flights that were delayed or not: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_df %>% \n  dplyr::count(is_delayed)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:   SQL [2 x 2]\n# Database: sqlite 3.39.4 [/Users/stephaniehicks/Documents/github/teaching/jhustatprogramming2022/data/nycflights13/nycflights13.sqlite]\n  is_delayed      n\n       <int>  <int>\n1          0 194078\n2          1 132897\n```\n:::\n:::\n\n\nThese classes are reasonably well balanced, but we going \nto use logistic regression, so I will load a perfectly \nbalanced sample of 40,000 data points.\n\nFor most databases, random sampling methods do not work\nsmoothly with R. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_df %>% \n  dplyr::sample_n(size = 1000)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in `dplyr::sample_n()`:\n! `tbl` must be a data frame, not a\n  `tbl_SQLiteConnection/tbl_dbi/tbl_sql/tbl_lazy/tbl` object.\n```\n:::\n:::\n\n\nSo it is not suggested to use `dplyr::sample_n()`\nor `dplyr::sample_frac()`. So we will have to be a little more manual.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\n# Create a modeling data set \ndf_mod <- flights_df %>%\n  # Within each class\n  group_by(is_delayed) %>%\n  # Assign random rank\n  mutate(x = random() %>% row_number()) %>%\n  ungroup()\n```\n:::\n\n\n:::{.callout-tip}\n\n### Note \n\n`dplyr::collect()` forces a computation of a database\nquery and retrieves data into a local tibble\n\nSo, here, we take the first 20K for each class for training set: \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_train <- df_mod %>%\n  group_by(is_delayed) %>%\n  filter(x <= 20000) %>%\n  collect() \n```\n:::\n\n\n:::\n\nThen, we take next 5K for test set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_test <- df_mod %>%\n  group_by(is_delayed) %>%\n  filter(x > 20000 & x <= 25000) %>%\n  collect() # again, this data is now loaded locally\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Double check I sampled right\ncount(df_train, is_delayed)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n# Groups:   is_delayed [2]\n  is_delayed     n\n       <int> <int>\n1          0 20000\n2          1 20000\n```\n:::\n\n```{.r .cell-code}\ncount(df_test, is_delayed)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n# Groups:   is_delayed [2]\n  is_delayed     n\n       <int> <int>\n1          0  5000\n2          1  5000\n```\n:::\n:::\n\n\nNow let's build a model – let's see if we can predict whether \nthere will be a delay or not by the combination of the\ncarrier, and the month of the flight.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"2022-12-20 13:12:00 EST\"\n```\n:::\n\n```{.r .cell-code}\nmod <- glm(is_delayed ~ carrier + as.factor(month),\n           family = \"binomial\", data = df_train)\nSys.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"2022-12-20 13:12:01 EST\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = is_delayed ~ carrier + as.factor(month), family = \"binomial\", \n    data = df_train)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.72211  -1.15578   0.02179   1.13832   1.65385  \n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        -0.03943    0.05590  -0.705 0.480531    \ncarrierAA          -0.19044    0.05568  -3.421 0.000625 ***\ncarrierAS          -0.57033    0.23897  -2.387 0.017005 *  \ncarrierB6           0.21380    0.05088   4.202 2.65e-05 ***\ncarrierDL          -0.19863    0.05190  -3.828 0.000129 ***\ncarrierEV           0.45207    0.05135   8.803  < 2e-16 ***\ncarrierF9           0.84999    0.22875   3.716 0.000203 ***\ncarrierFL           0.80119    0.11808   6.785 1.16e-11 ***\ncarrierMQ           0.33279    0.05739   5.798 6.70e-09 ***\ncarrierUA          -0.01874    0.05067  -0.370 0.711517    \ncarrierUS          -0.06820    0.06107  -1.117 0.264083    \ncarrierVX          -0.27751    0.09538  -2.909 0.003620 ** \ncarrierWN           0.22704    0.06862   3.309 0.000938 ***\ncarrierYV          -0.08512    0.24135  -0.353 0.724313    \nas.factor(month)2   0.03954    0.05227   0.756 0.449411    \nas.factor(month)3  -0.11045    0.04968  -2.223 0.026194 *  \nas.factor(month)4   0.15084    0.04990   3.023 0.002505 ** \nas.factor(month)5  -0.20930    0.04986  -4.198 2.69e-05 ***\nas.factor(month)6   0.20768    0.04985   4.166 3.10e-05 ***\nas.factor(month)7   0.22179    0.04937   4.493 7.04e-06 ***\nas.factor(month)8  -0.12508    0.04937  -2.534 0.011285 *  \nas.factor(month)9  -0.75668    0.05240 -14.440  < 2e-16 ***\nas.factor(month)10 -0.31364    0.04981  -6.296 3.04e-10 ***\nas.factor(month)11 -0.23873    0.05041  -4.736 2.18e-06 ***\nas.factor(month)12  0.41482    0.05027   8.252  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 55452  on 39999  degrees of freedom\nResidual deviance: 54066  on 39975  degrees of freedom\nAIC: 54116\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Out-of-Sample AUROC\ndf_test$pred <- predict(mod, newdata = df_test)\nauc <- suppressMessages(pROC::auc(df_test$is_delayed, df_test$pred))\nauc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArea under the curve: 0.6083\n```\n:::\n:::\n\n\nAs you can see, this is not a great model, but that's not the point here! \n\nInstead, we showed how to build a model on a small subset of a \nbig data set. Including sampling time, this took my laptop a\nsecond to run, making it easy to iterate quickly as I\nwant to improve the model. After I'm happy with this model, \nI could pull down a larger sample or even the entire data set\nif it is feasible, or do something with the model from the sample.\n\n\n# Chunk and Pull \n\nA second strategy to **chunk the data into separable units** and each\n**chunk is pulled separately and operated on serially**, in parallel, \nor after recombining. \n\nThis strategy is **conceptually similar** to \nthe [MapReduce algorithm](https://en.wikipedia.org/wiki/MapReduce) -- \nor _MapReduce is a framework using which we can write applications to process huge amounts of data, in parallel, on large clusters in a reliable manner_ -- \n[more here on MapReduce](https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm). \n\nDepending on the task at hand, the chunks might be time periods, \ngeographic units, or logical like separate businesses, departments, \nproducts, or customer segments.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://rviews.rstudio.com/post/2019-07-01-3-big-data-paradigms-for-r_files/chunk_pull.png)\n:::\n:::\n\n\n[[image source](https://rviews.rstudio.com/post/2019-07-01-3-big-data-paradigms-for-r_files/chunk_pull.png)]\n\n## Advantages\n\n- **Full data set**. The entire data set gets used.\n- **Parallelization**. If the chunks are run separately, the problem is easy to treat as [embarassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel) and make use of parallelization to speed runtimes.\n\n## Disadvantages\n\n- **Need Chunks**. Your data needs to have separable chunks for chunk and pull to be appropriate.\n- **Pull All Data**. Eventually have to pull in all data, which may still be very time and memory intensive.\n- **Stale Data**. The data may require periodic refreshes from the database to stay up-to-date since you’re saving a version on your local machine.\n\n## Example\n\nIn this case, I want to build another model of on-time arrival, \nbut I want to do it per-carrier. This is exactly the kind of use \ncase that is **ideal for chunk and pull**. \n\nI am going to separately pull the data in by carrier and run the\nmodel on each carrier's data.\n\nI am going to start by just getting the complete list of the carriers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get all unique carriers\ncarriers <- flights_df %>% \n  select(carrier) %>% \n  distinct() %>% \n  pull(carrier)\n\ncarriers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"9E\" \"AA\" \"AS\" \"B6\" \"DL\" \"EV\" \"F9\" \"FL\" \"MQ\" \"UA\" \"US\" \"VX\" \"WN\" \"YV\"\n```\n:::\n:::\n\n\nNow, I will write a function that\n\n- takes the name of a carrier as input\n- pulls the data for that carrier into R\n- splits the data into training and test\n- trains the model\n- outputs the out-of-sample AUROC (a common measure of model quality)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncarrier_model <- function(carrier_name) {\n  # Pull a chunk of data\n  df_mod <- flights_df %>%\n    filter(carrier == carrier_name) %>%\n    collect()\n  \n  # Split into training and test\n  split <- df_mod %>%\n    rsample::initial_split(prop = 0.9, strata = \"is_delayed\") %>% \n    suppressMessages()\n  \n  # Get training data\n  df_train <- split %>% \n                rsample::training()\n  \n  # Train model\n  mod <- glm(is_delayed ~ as.factor(month),\n             family = \"binomial\", data = df_train)\n  \n  # Get out-of-sample AUROC\n  df_test <- split %>% \n                rsample::testing()\n  df_test$pred <- predict(mod, newdata = df_test)\n  suppressMessages(auc <- pROC::auc(df_test$is_delayed ~ df_test$pred))\n  \n  auc\n}\n```\n:::\n\n\nNow, I am going to actually run the carrier model function \nacross each of the carriers. This code runs pretty quickly, \nand so I do not think the overhead of parallelization would be\nworth it. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nmods <- lapply(carriers, carrier_model) %>%\n  suppressMessages()\n\nnames(mods) <- carriers\n```\n:::\n\n\nLet's look at the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmods\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$`9E`\nArea under the curve: 0.5711\n\n$AA\nArea under the curve: 0.5731\n\n$AS\nArea under the curve: 0.5597\n\n$B6\nArea under the curve: 0.6208\n\n$DL\nArea under the curve: 0.5817\n\n$EV\nArea under the curve: 0.588\n\n$F9\nArea under the curve: 0.5134\n\n$FL\nArea under the curve: 0.5508\n\n$MQ\nArea under the curve: 0.572\n\n$UA\nArea under the curve: 0.6046\n\n$US\nArea under the curve: 0.5811\n\n$VX\nArea under the curve: 0.67\n\n$WN\nArea under the curve: 0.5607\n\n$YV\nArea under the curve: 0.6041\n```\n:::\n:::\n\n\nSo these models (again) are a little better than random chance. \nThe point was that we utilized the chunk and pull strategy to \npull the data separately by logical units and building a\nmodel on each chunk.\n\n\n# Push Compute to Data \n\nA third strategy is push some of the computing to where the data are \nstored before moving a subset of the data out of wherever it is stored\nand into R. \n\nImagine the data is compressed on a database somwhere. \nIt is often possible to obtain significant speedups simply by doing \nsummarization or filtering in the database before pulling the data into R.\n\nSometimes, more complex operations are also possible, including\ncomputing histogram and raster maps with \n[`dbplot`](https://db.rstudio.com/dbplot/), building a model with \n[`modeldb`](https://cran.r-project.org/web/packages/modeldb/index.html), and generating predictions from machine learning models with \n[`tidypredict`](https://db.rstudio.com/tidypredict/).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://rviews.rstudio.com/post/2019-07-01-3-big-data-paradigms-for-r_files/push_data.png)\n:::\n:::\n\n[[image source](https://rviews.rstudio.com/post/2019-07-01-3-big-data-paradigms-for-r_files/chunk_pull.png)]\n\n## Advantages\n\n- **Use the Database**. Takes advantage of what databases are often best at: quickly summarizing and filtering data based on a query.\n- **More Info, Less Transfer**. By compressing before pulling data back to R, the entire data set gets used, but transfer times are far less than moving the entire data set.\n\n## Disadvantages\n\n- **Database Operations**. Depending on what database you are using, some operations might not be supported.\n- **Database Speed**. In some contexts, the limiting factor for data analysis is the speed of the database itself, and so pushing more work onto the database is the last thing analysts want to do.\n\n## Example \n\nIn this case, I am doing a pretty simple BI task - plotting the \nproportion of flights that are late by the hour of departure \nand the airline.\n\nJust by way of comparison, let's run this first the naive \nway -– pulling all the data to my system and then doing my \ndata manipulation to plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time(\n  df_plot <- flights_df %>%\n    collect() %>%\n    group_by(carrier, sched_dep_time) %>%\n    # Get proportion per carrier-time\n    summarize(delay_pct = mean(is_delayed, na.rm = TRUE)) %>%\n    ungroup() %>%\n    # Change string times into actual times\n    dplyr::mutate(sched_dep_time = \n                    stringr::str_pad(sched_dep_time, 4, \"left\", \"0\") %>% \n             strptime(\"%H%M\") %>%  # converts character class into POSIXlt class\n             as.POSIXct()) # converts POSIXlt class to POSIXct class\n  ) -> timing1\n\ntiming1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  1.276   0.017   1.294 \n```\n:::\n:::\n\n\nNow that wasn't too bad, just 1.294 seconds on my laptop.\n\nBut let's see how much of a speedup we can get from chunk and pull. \nThe conceptual change here is significant - I'm doing as much \nwork as possible in the SQLite server now instead of locally. \n\nBut using `dplyr` means that the code change is minimal. The \nonly difference in the code is that the `collect()` call got\nmoved down by a few lines (to below `ungroup()`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time(\n  df_plot <- flights_df %>%\n    dplyr::group_by(carrier, sched_dep_time) %>%\n    # Get proportion per carrier-time\n    dplyr::summarize(delay_pct = mean(is_delayed, na.rm = TRUE)) %>%\n    dplyr::ungroup() %>%\n    dplyr::collect() %>%\n    # Change string times into actual times\n    dplyr::mutate(sched_dep_time = \n                    stringr::str_pad(sched_dep_time, 4, \"left\", \"0\") %>% \n             strptime(\"%H%M\") %>% \n             as.POSIXct())) -> timing2\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by \"carrier\". You can override using the\n`.groups` argument.\n```\n:::\n\n```{.r .cell-code}\ntiming2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  0.507   0.044   0.551 \n```\n:::\n:::\n\n\nIt might have taken you the same time to read this code as the \nlast chunk, but this took only 0.551 seconds to run, \nalmost an order of magnitude faster! That's pretty good for\njust moving one line of code.\n\nNow that we have done a speed comparison, we can create the \nnice plot we all came for.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_plot %>%\n  dplyr::mutate(carrier = paste0(\"Carrier: \", carrier)) %>%\n  ggplot(aes(x = sched_dep_time, y = delay_pct)) +\n    geom_line() +\n    facet_wrap(\"carrier\") +\n    ylab(\"Proportion of Flights Delayed\") +\n    xlab(\"Time of Day\") +\n    scale_y_continuous(labels = scales::percent) +\n    scale_x_datetime(date_breaks = \"4 hours\", \n                    date_labels = \"%H\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-30-1.png){width=90%}\n:::\n:::\n\n\nIt looks to me like flights later in the day might be \na little more likely to experience delays.\n\n# Summary \n\nThere are lots of ways you can work with large data in R. A \nfew that we learned about today include \n\n- Sample and model \n- Chunk and pull \n- Push compute to data\n\nHopefully this will help the next time you encounter a large \ndataset in R. \n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}