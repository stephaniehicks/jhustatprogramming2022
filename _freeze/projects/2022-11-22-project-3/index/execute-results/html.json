{
  "hash": "a766f98278a00a03e5b81ebc46e2be7e",
  "result": {
    "markdown": "---\ntitle: \"Project 3\"\nauthor: \n  - name: Stephanie Hicks\n    url: https://stephaniehicks.com\n    affiliation: Department of Biostatistics, Johns Hopkins\n    affiliation_url: https://publichealth.jhu.edu\ndescription: \"Building websites for R packages; practice functional programming and APIs\"\ndate: 2022-11-29\ncategories: [project 3, projects]\n---\n\n\n# Background\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n**Due date: December 13 at 11:59pm**\n\nThe goal of this assignment is to practice building websites for R packages, along with practice functional programming and using APIs. \n\n### To submit your project\n\nIn both parts below, you will need to create two separate github repositories for yourself. The links to create the repositories will be in CoursePlus. \n\nThe first one (Part 1) will be a public repository to build a website for an R package. It is public because you will need to deploy the website. \n\nThe second one (Part 2) will be a private repository to practice using two different APIs, practice functional programming, and building data analyses. \n\n# Part 1\n\nHere, we will practice using [`pkgdown`](https://pkgdown.r-lib.org). Using any R package with a GitHub repository (that does not already have a pkgdown website), use `pkgdown` to create a website for the software package. \n\n\n:::{.callout-note}\n\nThis could even been a package that you have written (or are working on creating right now). Otherwise, this could be a package that you have used previously or you can pick one you are not familiar with and just want to know more about! \n\nIt should **not** be the package you created in Project 2 for this course. \n\n:::\n\n## Part 1A: Create website locally \n\nFork the GitHub repository from the original location to your own GitHub account. Clone the repository to your local computer. \n\nUse `usethis` and `pkgdown` to create a website locally for the R package of your choice. \n\n## Part 1B: Customize the website\n\nHere, you need to customize the website in **at least 5 ways**. How you customize is up to you. The `pkgdown` website has lots of suggestions for you to try out! \n\n## Part 1C: Create an example data analysis\n\nIn this part, you will create a data analysis (or a case study) where you demonstrate the functions in the R package. Specifically, you will add [another article or vignette](https://pkgdown.r-lib.org/articles/customise.html#navbar) titled \"Example analysis\" inside the `/vignettes` folder.  \n\nSimilar to Project 2, you must pick out a data set from [TidyTuesday](https://www.tidytuesday.com) **that you have not worked with before** (i.e. not in a previous project or assignment from this class or from 776, but other classes or personal projects are acceptable). You must also demonstrate wrangling and plotting the data. Finally, your example analysis, must also demonstrate at least 2 functions from the R package in some way in the vignette. \n\nOther requirements for this part of vignette are the following: \n\n1. Pick any data set you wish from [TidyTuesday](https://www.tidytuesday.com) to analyze. \n  - You must describe what is the question you aim to answer with the data and data analysis. \n  - You must describe and link to where the original data come from that you chose.\n  - You must include a link to a data dictionary for the data or create one inside the webpage. \n2. Load the data into R\n  - In this step, you must test if a directory named `data` exists locally. If it does not, write an R function that creates it programmatically.  \n  - Saves the data only once (not each time you knit/render the document). \n  - Read in the data locally each time you knit/render. \n3. Your analysis must include some form of data wrangling and data visualization. \n  - You must use at least six different functions from `dplyr`, `tidyr`, `lubridate`, `stringr`, or `forcats`. \n  - You **must use at least two functions from `purrr`**. \n  - Your analysis should include at least three plots with you using at least three different `geom_*()` functions from `ggplot2` (or another package with `geom_*()` functions). \n    - Plots should have titles, subtitles, captions, and human-understandable axis labels. \n    - At least one plot should using a type of faceting (`facet_grid()` or `facet_wrap()`). \n4. Apply **at least 2 functions from the R package** in the vignette. \n5. Summarize and interpret the results in 1-2 sentences.\n6. At the end of the data analysis, list out each of the functions you used from each of the packages (`dplyr`, `tidyr`, `ggplot2`, etc) to help the TA with respect to making sure you met all the requirements described above. \n\n## Part 1D: Create a `README.md` file \n\nIf the package does not already include one, create and include a `README.md` file in the folder where the R package and pkgdown files are on your computer and add the following information below. \n\nIf it already has a `README.md` file, just edit the top of the file with the following information: \n\n- Include a URL to the GitHub link to where the original R package came from. \n- Include a URL to the deployed website that you will do in Part 1E, but it should be something like `https://jhu-statprogramming-fall-2022.github.io/biostat840-project3-pkgdown-<your_github_username>`.\n- Include a description of the 5 things you customized in your `pkgdown` website (excluding adding the example data analysis from Part 1C).\n\nThe readme must also include (if it does not already): \n\n- The title of package\n- The original author of the package (and you who made the website and example data analysis)\n- A goal / description of the package\n- A list of **exported** functions that are in the package. Briefly describe each function. \n- A basic example with one of the functions. \n\n## Part 1E: Deploy the website\n\nThe link to create a public GitHub repository for yourself to complete this part of Project 3 will be posted in CoursePlus. This creates an empty GitHub repository. \n\nWhen ready, deploy the website. \n\n:::{.callout-note}\n\nYou need to modify the template code that is provided to you from GitHub when you set the remote. There will already be a remote `origin` (from where you cloned the remote repository to your local repository), which you can see with \n\n``` {.bash filename=\"Bash\"}\ngit remote -v\n```\n\nTo change where you push your code, instead of (you will see this in the template code from GitHub when you create the public repository)\n\n``` {.bash filename=\"Bash\"}\ngit remote add origin <link>\n```\n\nYou want to use something like \n\n``` {.bash filename=\"Bash\"}\ngit remote add upstream <link>\n```\n\nand when you push your code, you want to use `git push -u upstream main`, for example (not `git push -u origin main`).\n\n:::\n\n# Part 2\n\nHere, we will practice using APIs and making data visualizations. \n\nFor this part of Project 3, you need to create a private GitHub repository for yourself, which will be posted in CoursePlus. This creates an empty GitHub repository. You need to show all your code and submit both the `.qmd` file and the rendered HTML file. \n\n:::{.callout-note}\n\nWhen you use an API, you want to figure out the data you want to extract and then save it locally so that you are not using the API each time you knit or render your data analysis. \n\nMost APIs have limits on the number of times you can ping it in a given hour and your IP address can be blocked if you try to ping it too many times within a short time. \n\n:::\n\n## Part 2A\n\nThe first API we will use is `tidycensus` (<https://walker-data.com/tidycensus>), which is an R package that allows users to interface with a select number of the US Census Bureau’s data APIs and return tidyverse-ready data frames, optionally with simple feature geometry included.\n\nThe goal of this part is to create a data analysis (or a case study) using the US Census Bureau’s data. \n\nOther requirements for this part are the following: \n\n1. You must describe what is the question you aim to answer with the data and data analysis. \n2. You must use at least three different calls to the `tidycensus` API to extract out different datasets. For example, these could be across years, locations, or variables. \n  - In this step, you must test if a directory named `data` exists locally. If it does not, write an R function that creates it programmatically.  \n  - Saves the data only once (not each time you knit/render the document). \n  - Read in the data locally each time you knit/render. \n  \n3. Your analysis must include some form of data wrangling and data visualization. \n  - You must use at least six different functions from `dplyr`, `tidyr`, `lubridate`, `stringr`, or `forcats`. \n  - You **must use at least two functions from `purrr`**. \n  - Your analysis should include at least three plots with you using at least three different `geom_*()` functions from `ggplot2` (or another package with `geom_*()` functions). \n    - Plots should have titles, subtitles, captions, and human-understandable axis labels. \n    - At least one plot should using a type of faceting (`facet_grid()` or `facet_wrap()`). \n4. Summarize and interpret the results in 1-2 sentences.\n5. At the end of the data analysis, list out each of the functions you used from each of the packages (`dplyr`, `tidyr`, `ggplot2`, etc) to help the TA with respect to making sure you met all the requirements described above. \n\n## Part 2B \n\nThe second API we will use is the [Covid Act Now Data API](https://covidactnow.org/data-api). \n\nThe goal of this part is to create a data analysis (or a case study) using the US Census Bureau’s data. \n\nOther requirements for this part are the following: \n\n1. You must describe what is the question you aim to answer with the data and data analysis. \n2. You must use at least three different calls to the Covid Act Now Data API to extract out different datasets. For example, these could be across counties, etc. \n  - In this step, you must test if a directory named `data` exists locally. If it does not, write an R function that creates it programmatically.  \n  - Saves the data only once (not each time you knit/render the document). \n  - Read in the data locally each time you knit/render. \n3. Your analysis must include some form of data wrangling and data visualization. \n  - You must use at least six different functions from `dplyr`, `tidyr`, `lubridate`, `stringr`, or `forcats`. \n  - You **must use at least two functions from `purrr`**. \n  - Your analysis should include at least three plots with you using at least three different `geom_*()` functions from `ggplot2` (or another package with `geom_*()` functions). \n    - Plots should have titles, subtitles, captions, and human-understandable axis labels. \n    - At least one plot should using a type of faceting (`facet_grid()` or `facet_wrap()`). \n4. Summarize and interpret the results in 1-2 sentences.\n5. At the end of the data analysis, list out each of the functions you used from each of the packages (`dplyr`, `tidyr`, `ggplot2`, etc) to help the TA with respect to making sure you met all the requirements described above. \n6. Push your code and rendered HTML to the private repository that you created for yourself. \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}