---
title: "Relational databases and SQL basics"
author: 
  - name: Stephanie Hicks
    url: https://stephaniehicks.com
    affiliation: Department of Biostatistics, Johns Hopkins
    affiliation_url: https://publichealth.jhu.edu
description: "Introduction to relational databases and SQL"
date: 2022-12-08
draft: true
categories: [module 3, week 7, databases, SQL]
---

<!-- Add interesting quote -->

# Pre-lecture materials

### Read ahead

::: callout-note
## Read ahead

**Before class, you can prepare by reading the following materials:**

1.  Add here.
2.  Add here.
:::

### Acknowledgements

Material for this lecture was borrowed and adopted from

-   Add here.

# Learning objectives

::: callout-note
# Learning objectives

**At the end of this lesson you will:**

-   Add here.
:::

# Add lecture here

Before we begin, you will need to install
these packages

```{r}
#| eval: false
install.packages("DBI")
install.packages("RSQLite")
install.packages("dbplyr")
```

Now we load a few R packages
```{r}
#| warning: false
#| message: false
library(tidyverse)
library(DBI)
library(RSQLite)
library(dbplyr)
```

## Reading in from `SQLite` database

Another important type of data you might interact with 
are databases (such as `SQL` or `SQLite`). There are several ways to 
[query databases in R](https://db.rstudio.com/getting-started/database-queries/). 

First, we will download a `.sqlite` database. This is a
portable version of a `SQL` database. For our 
purposes, we will use the 
[chinook sqlite database here](https://github.com/lerocha/chinook-database/blob/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite). The database represents a 
"digital media store, including tables for artists, 
albums, media tracks, invoices and customers".

From the [Readme.md](https://github.com/lerocha/chinook-database) file: 

> Sample Data
> 
> Media related data was created using real data from an iTunes Library. It is possible for you to use your own iTunes Library to generate the SQL scripts, see instructions below. Customer and employee information was manually created using fictitious names, addresses that can be located on Google maps, and other well formatted data (phone, fax, email, etc.). Sales information is auto generated using random data for a four year period.

```{r}
library(here)
if(!file.exists(here("data", "Chinook.sqlite"))){
  file_url <- paste0("https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite")
  download.file(file_url,
                destfile=here("data", "Chinook.sqlite"))
}
list.files(here("data"))
```

The main workhorse packages that we will use are 
the `DBI` and `dplyr` packages. Let's look at the 
`DBI::dbConnect()` help file

```{r, eval=FALSE}
?DBI::dbConnect
```

So we need a driver and one example is `RSQLite::SQLite()`. 
Let's look at the help file

```{r, eval=FALSE}
?RSQLite::SQLite
```

Ok so with `RSQLite::SQLite()` and `DBI::dbConnect()` 
we can connect to a `SQLite` database. Let's try that 
with our `Chinook.sqlite` file that we downloaded. Chinook.sqlite

```{r}
library(DBI)
conn <- DBI::dbConnect(RSQLite::SQLite(), 
                       here("data", "Chinook.sqlite"))
conn
```

So we have opened up a connection with the SQLite database. 
Next, we can see what tables are available in the database 
using the `dbListTables()` function: 

```{r}
dbListTables(conn)
```

From RStudio's website, there are several ways to interact with 
SQL Databases. One of the simplest ways that we will use here is 
to leverage the `dplyr` framework. 

> "The `dplyr` package now has a generalized SQL backend for talking to databases, and the new `dbplyr` package translates R code into database-specific variants. As of this writing, SQL variants are supported for the following databases: Oracle, Microsoft SQL Server, PostgreSQL, Amazon Redshift, Apache Hive, and Apache Impala. More will follow over time.

So if we want to query a SQL databse with `dplyr, the 
benefit of using `dbplyr` is: 

> "You can write your code in `dplyr` syntax, and `dplyr` will translate your code into SQL. There are several benefits to writing queries in `dplyr` syntax: you can keep the same consistent language both for R objects and database tables, no knowledge of SQL or the specific SQL variant is required, and you can take advantage of the fact that `dplyr` uses lazy evaluation.

Let's take a closer look at the `conn` database
that we just connected to:

```{r}
library(dbplyr)
src_dbi(conn)
```

You can think of the multiple tables similar to having 
multiple worksheets in a spreadsheet. 

Let's try interacting with one. 

### Querying with `dplyr` syntax

First, let's look at the first ten rows in the 
`Album` table. 
```{r}
tbl(conn, "Album") %>%
  head(n=10)
```

The output looks just like a `data.frame` that we are familiar 
with. But it's important to know that it's not really 
a dataframe. For example, what about if we use 
the `dim()` function? 
```{r}
tbl(conn, "Album") %>%
  dim()
```

Interesting! We see that the number of rows returned is `NA`. 
This is because these functions are different than operating 
on datasets in memory (e.g. loading data into memory using 
`read_csv()`). Instead, `dplyr` communicates differently 
with a SQLite database. 

Let's consider our example. If we were to use straight SQL, 
the following SQL query returns the first 10 rows 
from the `Album` table:

```{r, eval=FALSE}
SELECT *
FROM `Album`
LIMIT 10
```

In the background, `dplyr` does the following: 

* translates your R code into SQL
* submits it to the database
* translates the database's response into an R data frame

To better understand the `dplyr` code, we can use the 
`show_query()` function: 
```{r}
Album <- tbl(conn, "Album")
show_query(head(Album, n = 10))
```

This is nice because instead of having to write the 
SQL query ourself, we can just use the `dplyr` and R 
syntax that we are used to. 

However, the downside is that `dplyr` never gets to see the 
full `Album` table. It only sends our query to the database, 
waits for a response and returns the query. However, in this 
way we can interact with large datasets! 

Many of the usual `dplyr` functions are available too: 

* `select()`
* `filter()`
* `summarize()` 

and many join functions. 

Ok let's try some of the functions out. 
First, let's count how many albums each 
artist has made. 

```{r}
tbl(conn, "Album") %>%
  group_by(ArtistId) %>% 
  summarize(n = count(ArtistId)) %>% 
  head(n=10)
```

Next, let's plot it. 
```{r}
tbl(conn, "Album") %>%
  group_by(ArtistId) %>% 
  summarize(n = count(ArtistId)) %>% 
  arrange(desc(n)) %>% 
  ggplot(aes(x = ArtistId, y = n)) + 
  geom_bar(stat = "identity")
```

Let's also extract the first letter from each 
album and plot the frequency of each letter. 

```{r}
tbl(conn, "Album") %>%
  mutate(first_letter = str_sub(Title, end = 1)) %>% 
  ggplot(aes(first_letter)) + 
  geom_bar()
```




# Post-lecture materials

### Final Questions

Here are some post-lecture questions to help you think about the material discussed.

::: callout-note
### Questions

1.  Add here.
:::

### Additional Resources

::: callout-tip
-   Add here.
:::

